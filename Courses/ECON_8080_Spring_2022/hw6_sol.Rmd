---
output:  pdf_document
fontsize: 11pt
indent: true
header-includes:
 - \usepackage{amsmath,amssymb,setspace}
 - \newcommand{\E}{\textrm{E}}
 - \newcommand{\var}{\textrm{var}}
 - \newcommand{\cov}{\textrm{cov}}
 - \newcommand{\corr}{\textrm{corr}}
 - \newcommand{\independent}{\perp}
 - \usepackage[margin=1in]{geometry}
 - \usepackage{setspace}
 - \usepackage[most]{tcolorbox}
 - \newtcolorbox{discussionbox}[1][]{sharp corners, enhanced, colback=white, attach title to upper,#1}
 - \usepackage{bm}
---

\onehalfspacing

# Homework 6 Solutions

## Hansen 17.2

$\E[e_{it}|X_{it}]=0$ is not a strong enough condition for $\hat{\beta}$ from a fixed effects regression to be unbiased.  This condition says that $e_{it}$ is (mean) independent of $X_{it}$, but it does not rule out that $e_{it}$ could be related to, say, $X_{it+1}$.  This is not an entirely strange case either, particularly if a good "shock" in the current period leads to the covariate changing in the next time period.

More specifically, recall that
\begin{align*}
  \E[\hat{\beta} - \beta | \mathbf{X}] &= \left( \sum_{i=1}^n \dot{\mathbf{X}}_i' \dot{\mathbf{X}}_i \right)^{-1} \sum_{i=1}^n \dot{\mathbf{X}}_i' \E[\mathbf{e}_i | \mathbf{X}] \\
  &= \left( \sum_{i=1}^n \dot{\mathbf{X}}_i' \dot{\mathbf{X}}_i \right)^{-1} \sum_{i=1}^n \dot{\mathbf{X}}_i' \E[\mathbf{e}_i | \mathbf{X}_i]
\end{align*}
where $\mathbf{X}$ is the $nT \times k$ "data matrix" and the other notation is from class, and the second equality uses that the observations are independent of each other.  Notice that,
\begin{align*}
  \E[\mathbf{e}_i | \mathbf{X}_i] = \begin{bmatrix} \E[e_{i1}|X_{i1}, X_{i2}, \ldots, X_{iT}] \\
  \E[e_{i2}|X_{i1}, X_{i2}, \ldots, X_{iT}] \\
  \vdots \\
  \E[e_{iT}|X_{i1}, X_{i2}, \ldots, X_{iT}]\end{bmatrix}
\end{align*}
The condition in the problem is not strong enough that this term is equal to 0.  And, if it is some function of $X$, then $\hat{\beta}$ would not, in general, be unbiased for $\beta$.

## Additional Question 2

Yes, they are correct.  Notice that, because there are only two periods, this regression is equivalent to
\begin{align*}
  \Delta Y_{i2} = \Delta \theta_2 + \alpha D_{i2} + \Delta e_{i2}
\end{align*}
Moreover,
\begin{align}
  \E[\Delta Y_2 | D_2=1] &= \Delta \theta_2 + \alpha \label{eqn:1} \\
  \E[\Delta Y_2 | D_2=0] &= \Delta \theta_2 \label{eqn:2}
\end{align}
which implies that
\begin{align*}
  \alpha &= \E[\Delta Y_2 | D_2=1] - \E[\Delta Y_2 | D_2=0] \\
  &= \E[\Delta Y_2 | D_2=1] - \E[\Delta Y_2(0) | D_2=0] \\
  &= \E[\Delta Y_2 | D_2=1] - \E[\Delta Y_2(0) | D_2=1] \\
  &= ATT
\end{align*}
where the first line subtracts Equation \ref{eqn:2} from Equation \ref{eqn:1}, the second equality writes the second term in terms of potential outcomes, and the third equality holds by the parallel trends assumption.

## Additional Question 2

```{r, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
library(randomForest)
data <- as.data.frame(haven::read_dta("jtrain_observational.dta"))

n <- nrow(data)
data$id <- 1:n
fold1 <- subset(data, (id%%2) == 0)
fold2 <- subset(data, (id%%2) == 1)

ml_att <- function(f1, f2) {
  
  # use f1 to estimate the first step models
  Dmod <- randomForest(as.factor(train) ~ age + educ + black + hisp +
                         married + re75 + unem75, data=f1)
  Ymod <- randomForest(re78 ~ age + educ + black + hisp + 
                         married + re75 + unem75, data=f1)
  
  # get predictions with f2
  pscore <- predict(Dmod, newdata=f2, type="prob")[,2] # this gets p(d=1|x)
  out_reg <- predict(Ymod, newdata=f2)
  
  # compute att(k) with f2
  D <- f2$train
  p <- mean(D)
  Y <- f2$re78
  att1 <- mean(D/p*(Y-out_reg))
  att2 <- mean((1-D)/p * pscore/(1-pscore) * (Y-out_reg) )
  att <- att1-att2
  att
}

# cross splitting
ml1 <- ml_att(fold1,fold2)
# reverse roles
ml2 <- ml_att(fold2,fold1)
# average
mean(c(ml1,ml2))
```

This is larger than what we estimated on the last homework (about 0.85) though I noticed that my estimates do move somewhat if I run the code multiple times.    

## Additional Question 3

### Part (a)

```{r}
library(Matrix)
load("job_displacement_clean2.RData")
# drop already treated
data <- subset(data, first.displaced != 2001)
data <- droplevels(data)
Y <- data$learn
data$D <- 1*( (data$year >= data$first.displaced) & data$first.displaced != 0)
X <- model.matrix(~ as.factor(year) + D, data=data)
n <- length(unique(data$id))
tp <- length(unique(data$year))
iT <- matrix(rep(1,tp))
Dmat <- bdiag(replicate(n,iT,simplify=FALSE))
M <- Matrix::Diagonal(n*tp) - Dmat%*%solve(t(Dmat)%*%Dmat)%*%t(Dmat)
bet <- solve(t(X)%*%M%*%X) %*% t(X)%*%M%*%Y
bet
```

Next, let's calculate the standard errors where we use that
\begin{align*}
  \sqrt{n}(\hat{\beta}-\beta) &= \E[\mathbf{X}_i'\mathbf{M}_i \mathbf{X}_i]^{-1} \frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{X}_i'\mathbf{M}_i \mathbf{e}_i + o_p(1) \\
  & \xrightarrow{d} N(0,\mathbf{V})
\end{align*}
where
\begin{align*}
  \mathbf{V} &= \E[\mathbf{X}_i'\mathbf{M}_i \mathbf{X}_i]^{-1} \mathbf{\Omega} \E[\mathbf{X}_i'\mathbf{M}_i \mathbf{X}_i]^{-1} \\
  &= \E[\dot{\mathbf{X}}_i'\dot{\mathbf{X}}_i]^{-1} \mathbf{\Omega} \E[\dot{\mathbf{X}}_i'\dot{\mathbf{X}}_i]^{-1}
\end{align*}
and
\begin{align*}
  \mathbf{\Omega} &= \E[\mathbf{X}_i'\mathbf{M}_i \mathbf{e}_i \mathbf{e}_i' \mathbf{M}_i \mathbf{X}_i] \\
  &= \E[\dot{\mathbf{X}}_i' \mathbf{e}_i \mathbf{e}_i' \dot{\mathbf{X}}_i] 
\end{align*}
It's worth thinking about how to actually estimate these because $\dot{\mathbf{X}}_i$ is a matrix rather than our usual case of it being a vector.  First, notice that
\begin{align*}
  \dot{\mathbf{X}}_i' \dot{\mathbf{X}}_i = \sum_{t=1}^T \dot{X}_{it}  \dot{X}_{it}'
\end{align*}
which is a $k\times k$ matrix.  Thus, the natural estimate of $\E[\dot{\mathbf{X}}_i' \dot{\mathbf{X}}_i]$ is
\begin{align*}
  \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^T \dot{X}_{it} \dot{X}_{it}' = \dot{\mathbf{X}}'\dot{\mathbf{X}} / n
\end{align*}
which corresponds to exactly the same way that we estimated this type of term throughout the semester.  Next,
\begin{align*}
  \dot{\mathbf{X}}_i' \mathbf{e}_i = \sum_{t=1}^T X_{it} e_{it}
\end{align*}
which is a $k\times 1$ vector and so that 
\begin{align*}
  \dot{\mathbf{X}}_i' \mathbf{e}_i \mathbf{e}_i' \dot{\mathbf{X}}_i = \left( \sum_{t=1}^T \dot{X}_{it} e_{it} \right) \left( \sum_{t=1}^T \dot{X}_{it} e_{it} \right)'
\end{align*}
and implies that we would estimate $\mathbf{\Omega}$ by 
\begin{align*}
  \hat{\mathbf{\Omega}} = \frac{1}{n}\sum_{i=1}^n \left( \sum_{t=1}^T \dot{X}_{it} \hat{e}_{it} \right) \left( \sum_{t=1}^T \dot{X}_{it} \hat{e}_{it} \right)'
\end{align*}
As far as I know, you can't play the same matrix algebra "trick" that we usually use here (in particular, recall that in the cross sectional case we could estimate $\hat{\mathbf{\Omega}} = \frac{1}{n} \sum_{i=1}^n X_i X_i' \hat{e}_i^2$, but that, for programming, it was often convenient to re-express this $\hat{\mathbf{\Omega}} = \tilde{\mathbf{X}}' \tilde{\mathbf{X}}/n$ where a typical element of $\tilde{\mathbf{X}}$ is given by $X_i \hat{e}_i$.)  Anyway, the line below that uses the `rowsum` function is essentially just manually calculating $\sum_{t=1}^T X_{it} \hat{e}_{it}$ and then using matrix algebra below it.


```{r}
ehat <- as.numeric(Y - X%*%bet)
n <- length(unique(data$id))
dotX <- M%*%X
Q <- t(X) %*% dotX / n
dotXe <- rowsum(as.matrix(dotX*ehat), group=data$id)
Omeg <- t(dotXe)%*%dotXe/n
V <- solve(Q)%*%Omeg%*%solve(Q)
se <- sqrt(diag(V))/sqrt(n)
round(cbind.data.frame(bet=as.numeric(bet), se=se), 4)
```


Thus, we estimate that job displacement reduces earnings by about 23\%.  As a check, let's compare this to what we get from `fixest`.

```{r}
library(fixest)
fe_reg <- feols(learn ~ as.factor(year) + D | id, data=data)
summary(fe_reg)
```

These appear to be the same (or the same up to possibly a degree of freedom adjustment).

### Part (b)

```{r}
# list of time periods
# for simplicity I'm going to convert this to 1,2,3,4,5,6,7...
tlist <- (sort(unique(data$year)) - 1999)/2
# list of groups (excluding never-treated)
glist <- (sort(unique(data$first.displaced))[-1] - 1999)/2

# create new variables in updated time scale
data$G <- ifelse(data$first.displaced==0, 0, (data$first.displaced - 1999)/2)
data$tp <- (data$year-1999)/2


# write a function to compute att(g,t)
# I compute these as averages using weights, but it 
# is fine to use subsets of data here too.
# @param w weights, used for bootstrap
# @param base_period, allows base period to optionally 
#  be fixed at one
compute.attgt <- function(data, w=rep(1,nrow(data)),
                          use_base_period_1=FALSE) {
  # data frame to store results
  results <- list()
  counter <- 1
  
  for (this_t in tlist[-1]) {
    for (this_g in glist) {
      base_period <- min(this_t-1, this_g-1)
      if (use_base_period_1) base_period <- 1
      G <- 1*(data$G==this_g)
      U <- 1*(data$G==0)
      pre <- 1*(data$tp == base_period)
      post <- 1*(data$tp == this_t)
      pg <- weighted.mean(data$G == this_g, w=w)
      pu <- weighted.mean(data$G == 0, w=w)
      ppre <- mean(pre)
      ppost <- mean(post)
      
      this_attgt <- weighted.mean(data$learn*G*post/pg/ppost, w=w) -
                          weighted.mean(data$learn*G*pre/pg/ppre, w=w) - 
        (weighted.mean(data$learn*U*post/pu/ppost, w=w) -
           weighted.mean(data$learn*U*pre/pu/ppre, w=w))
      results[[counter]] <- c(attgt=this_attgt, g=this_g, t=this_t)
      
      counter <- counter+1
    }
  }
  
  # convert to data frame
  results <- as.data.frame(do.call("rbind", results))
  
  results
}

results <- compute.attgt(data)
# print results
round(results[order(results$g, results$t),],4)
```


### Part (c)

```{r}
# function to compute attO
# ret_weights argument optionally returns the underlying
# weights rather than attO
compute.attO <- function(attgt_results, w=rep(1,nrow(data)),
                         ret_weights=FALSE) {
  # overall attgt weights
  ever_treated <- which(data$G != 0)
  w <- w[ever_treated]
  pg <- sapply(glist, function(g) weighted.mean(data[ever_treated,]$G==g, w=w))
  maxT <- max(tlist)
  wO <- function(g,t) {
    1*(t >= g)*pg[glist==g] / (maxT - g + 1)
  }
  # add weights to results
  wOgt <- sapply(1:nrow(attgt_results), 
                 function(i) wO(attgt_results$g[i], attgt_results$t[i]))
  attgt_results$wO <- wOgt
  # optionally return computed weights
  if(ret_weights) return(attgt_results)
  attO <- sum(attgt_results$attgt*attgt_results$wO)
  attO
}

attO <- compute.attO(results)

# bootstrap standard errors
B <- 100
id_list <- unique(data$id)
boot_attO <- list()
for (b in 1:B) {
  # draw weights from multinomial distribution (this is exactly the same
  # as empirical bootstrap)
  boot_weights <- as.numeric(rmultinom(n=1, size=n, prob=rep(1/n,n)))
  this_boot_weights_id <- cbind.data.frame(id=id_list, boot_weights=boot_weights)
  boot_data <- merge(data, this_boot_weights_id, by="id")
  boot_attgt <- compute.attgt(data, w=boot_data$boot_weights)
  boot_attO[[b]] <- compute.attO(boot_attgt, w=boot_data$boot_weights)
}

boot_attO <- do.call("rbind", boot_attO)
se <- sd(boot_attO)

round(cbind.data.frame(attO=attO, se=se), 4)
```

Thus, we are estimating a large, negative and statistically significant effect of job displacement. 

### Part (d)

```{r}
# twfe weights
Edt <- function(t) {
  mean( (t >= data$G) & (data$G !=0) )
}
mEdt <- mean(sapply(tlist, Edt))
pg2 <- sapply(glist, function(g) mean(data$G==g))
maxT <- max(tlist)
wTWFE_num <- function(g,t,post0=FALSE) {
  if ((t < g) & post0) return(0) 
  hgt <- 1*(t>=g) - (maxT-g+1)/maxT - Edt(t) + mEdt
  hgt*pg2[glist==g]
}
# add weights to results
wTWFEgt_num <- sapply(1:nrow(results), 
                      function(i) wTWFE_num(results$g[i],
                                            results$t[i],
                                            post0=TRUE))
wTWFEgt_den <- sapply(1:nrow(results), 
                      function(i) wTWFE_num(results$g[i],
                                            results$t[i],
                                            post0=TRUE))
wTWFEgt <- wTWFEgt_num/sum(wTWFEgt_den)
results$wTWFE <- wTWFEgt

# get results from attO
results$wattO <- compute.attO(results, ret_weights=TRUE)$wO

# print results
round(results[order(results$g, results$t),], 4)
```

Notice that none of the TWFE weights are negative here though some of them do seem fairly different from the weights on $ATT^O$.

As a final side-comment, you might notice that

```{r}
attTWFE <- sum(results$attgt*results$wTWFE)
attTWFE
```

is not exactly equal to $\alpha$ that we calculated earlier.  There are two reasons for this.  First, our expression for $\alpha$ in terms of underlying $ATT(g,t)$'s relied on parallel trends actually holding; so if it does not, then we will not get exactly the same thing.  Second, there is estimation error in $ATT(g,t)$; that is, we have $\widehat{ATT}(g,t)$ rather than $ATT(g,t)$, and the way to estimate this is not unique.  Let me very quickly give show how you can recover $\alpha$.  In the notes, the line right before relating $\alpha$ to underlying $ATT(g,t)$'s was
\begin{align*}
  \alpha = \sum_{t=2}^T \sum_{g \in \bar{\mathcal{G}}} h(g,t)  \Big( \E[Y_{it} - Y_{i1}) | G=g] - \E[Y_{it} - Y_{i1}) | U=0]\Big)  p_g \Bigg/ \sum_{t=1}^T \E[\ddot{D}_{it}^2]
\end{align*}
We can use this to compute a "decomposition" of $\alpha$ that will be equal to what we actually estimated.

```{r}
# this computes "ATT(g,t)'s" using base period = 1 
# everywhere which is analogous to above equation
results2 <- compute.attgt(data, use_base_period_1=TRUE)
# compute weights but allow for non-zero weights 
# in pre-treatment periods 
wTWFEgt_num2 <- sapply(1:nrow(results), 
                       function(i) wTWFE_num(results$g[i],
                                             results$t[i],
                                             post0=FALSE))
wTWFEgt_den2 <- sapply(1:nrow(results), 
                       function(i) wTWFE_num(results$g[i],
                                             results$t[i],
                                             post0=TRUE))
wTWFEgt2 <- wTWFEgt_num2/sum(wTWFEgt_den2)
# check if this delivers alpha
sum(results2$attgt*wTWFEgt2)
```

which are now the same.

### Part (e)

```{r}
# function to compute event studies
compute.es <- function(attgt_results, w=rep(1,nrow(data))) {
  # event study weights
  eseq <- sort(unique(attgt_results$t - attgt_results$g))
  es_res <- list()
  counter <- 1
  for (e in eseq) {
    this_keepers <- which( (attgt_results$t - attgt_results$g) == e)
    this_attgt <- attgt_results$attgt[this_keepers]
    pg <- sapply(attgt_results$g[this_keepers], 
                 function(g)  weighted.mean(data$G==g, w=w))
    pg <- pg / sum(pg)
    att_e <- sum(this_attgt*pg)
    es_res[[counter]] <- c(att_e=att_e, e=e)
    counter <- counter+1
  }
  # convert to data frame
  es_results <- as.data.frame(do.call("rbind", es_res))
  es_results
}   

es_results <- compute.es(results)

# bootstrap event study
B <- 100
id_list <- unique(data$id)
boot_es <- list()
for (b in 1:B) {
  boot_weights <- as.numeric(rmultinom(n=1, size=n, prob=rep(1/n,n)))
  this_boot_weights_id <- cbind.data.frame(id=id_list, boot_weights=boot_weights)
  boot_data <- merge(data, this_boot_weights_id, by="id")
  boot_attgt <- compute.attgt(data, w=boot_data$boot_weights)
  boot_es[[b]] <- compute.es(boot_attgt, w=boot_data$boot_weights)$att_e
}

boot_es <- do.call("rbind", boot_es)
se <- apply(boot_es, 2, sd)
es_results$se <- se
es_results$ciL <- es_results$att_e - 1.96*es_results$se
es_results$ciU <- es_results$att_e + 1.96*es_results$se

library(ggplot2)
ggplot(data=es_results, mapping=aes(x=e,y=att_e)) +
  geom_line() +
  geom_point(size=1.5) +
  geom_line(aes(y=ciU), linetype="dotted") + 
  geom_line(aes(y=ciL), linetype="dotted") + 
  scale_x_continuous(breaks=seq(-5,5), labels=seq(-10,10,2)) +
  theme_bw()
```

The figure suggests that job displacement causes earnings to drop by, on average, about 20% and that this effect is quite persistent; it appears to be roughly the same 10 years following job displacement.  If you look at the estimates in pre-treatment periods, with the exception of 10 years before job displacement, the estimates are fairly close to 0 (and not statistically different from 0) suggesting that the parallel trends assumption is likely to be fairly reasonable in this application.
