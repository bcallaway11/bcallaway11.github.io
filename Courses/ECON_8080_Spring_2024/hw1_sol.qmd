---
format:  pdf
fontsize: 11pt
indent: true
header-includes:
 - \input{../preamble.tex}
---

# Homework 1 Solutions

**Hansen 2.2**

\begin{align*}
  \E[YX] &= \E[X \E[Y|X]] \\
  &= \E[X (a+bX)] \\
  &= \E[aX + bX^2] \\
  &= a\E[X] + b\E[X^2]
\end{align*}
where the first equality holds by the law of iterated expectations, the second equality holds by the expression for $\E[Y|X]$ in the problem, and the remaining two equalities are just algebra/basic properties of expectations.


## Hansen 2.5

a) The mean squared error is given by
\begin{align*}
  MSE = \E[(e^2 - h(X))^2]
\end{align*}    

b) $e^2$ is closely related to a measure of the magnitude of how far off our predictions of $Y$ given $X$ are.  For example, given $X$, if we predict a "high" value of $e^2$, it would suggest that we expect our predictions of $Y$ to not be too accurate for that value of $X$.

c) Recall that $\sigma^2(X) = \E[e^2|X]$ so that
    \begin{align*}
      MSE &= \E\Big[((e^2 - \E[e^2|X]) - (h(X) - \E[e^2|X]))^2\Big] \\
      &= \E\Big[(e^2 - \E[e^2|X])^2\Big] - 2\E\Big[(e^2 - \E[e^2|X])(h(X) - \E[e^2|X])\Big] + \E\Big[(h(X) - \E[e^2|X])^2\Big]
    \end{align*}
    Let's consider each of these three terms.

    * The first term does not depend on $h(X)$ so it is invariant to our choice of $h$.
    
    * The second term is equal to 0 after applying the law of iterated expectations.
    
    * The third term is minimized by setting $h(X) = \E[e^2|X] = \sigma^2(X)$ which implies that $MSE$ is minimized by $\sigma^2(X)$.

## Hansen 2.6

To start with, notice that
\begin{align*}
  \E[Y] = \E[m(X) + e] = \E[m(X)]
\end{align*}
where the last equality holds because $\E[e]=0$.  Thus, we have that
\begin{align*}
  \var(Y) &= \E\Big[(Y - \E[Y])^2\Big] \\
  &= \E\Big[(m(X) + e - \E[m(X)])^2\Big] \\
  &= \E\Big[(m(X) - \E[m(X)])^2\Big] + 2\E\Big[(m(X) - \E[m(X)])e\Big] + \E[e^2] \\
  &= \E\Big[(m(X) - \E[m(X)])^2\Big] + \E[e^2] \\
  &= \var[m(X)] + \sigma^2
\end{align*}
where first equality holds by the definition of $\var(Y)$, the second equality holds by the expression for $\E[Y]$ in the previous display, the third equality holds by expanding the square, the fourth equality holds by applying the law of iterated expectations to the middle term (and because $\E[e|X]=0$), and the fifth equality holds by the definition of $\var[m(X)]$ and the fact that $\E[e^2]=\sigma^2$.

## Hansen 2.10

True. 

\begin{align*}
  \E[X^2e] = \E\big[ X^2 \underbrace{\E[e|X]}_{=0} \big] = 0
\end{align*}

## Hansen 2.11

False.  Here is a counterexample.  Suppose that $X=1$ with probability $1/2$ and that $X=-1$ with probability $1/2$.  Importantly, this means that $X^2=1$, $X^3=X$, $X^4=1$, and so on; this further implies that $\E[X]=0$, $\E[X^2]=1$, $\E[X^3]=0$ and so on.  Also, suppose that $\E[e|X] = X^2$.  Then, $\E[Xe] = \E[X\E[e|X]] = \E[X \cdot X^2] = \E[X^3] = 0$.  However, $\E[X^2e] = \E[X^2\E[e|X]] = \E[X^2 \cdot X^2] = \E[X^4] = 1 \neq 0$

## Hansen 2.12

False.  Here is a counterexample.  Suppose that $\E[e^2|X]$ depends on $X$, then $e$ and $X$ are not independent.  As a concrete counterexample, suppose $e|X \sim \N(0,X^2)$ (that is, conditional on $X$, $e$ follows a normal distribution with mean 0 and variance $X^2$).  In this case $\E[e|X]=0$, but $e$ and $X$ are not independent.

## Hansen 2.13

False.  The same counterexample as in 2.11 works here.  In that case, $\E[Xe]=0$, but $\E[e|X]=X^2$ (in that case $X^2 = 1$, but the main point is that it is not equal to 0 for all values of $X$).

## Hansen 2.14

False.  In this case, higher order moments can still depend on $X$.  For example, $\E[e^3|X]$ can still depend on $X$.  If it does, then $e$ and $X$ are not independent.

## Hansen 2.21

a) Following omitted variable bias types of arguments (also, notice that the notation in the problem implies that $X$ is scalar here), we have that
\begin{align*}
  \gamma_1 &= \frac{\E[XY]}{\E[X^2]} \\
  &= \frac{\E[X(X\beta_1 + X^2 \beta_2 + u)]}{\E[X^2]} \\
  &= \beta_1 + \frac{\E[X^3]}{\E[X^2]}\beta_2
\end{align*}

    Thus, $\gamma_1=\beta_1$ if either $\beta_2=0$ or $\E[X^3] = 0$.  $\beta_2=0$ if $X^2$ does not have an effect on the outcome (after accounting for the effect of $X$); this is similar to the omitted variable logic that we talked about in class.  A leading case where $\E[X^3]=0$ is when $X$ is a mean 0 symmetric random variable; for example, if $X$ is standard normal, then its third moment is equal to 0.

b) Using the same arguments as in part (a), we have that
\begin{align*}
  \gamma_1 = \theta_1 + \frac{\E[X^4]}{\E[X^2]} \theta_2
\end{align*}

    Similar to the previous part, $\gamma_1$ could equal $\theta_1$ if $\theta_2$ were equal to 0.  Unlike the previous part though, here, we cannot have that $\E[X^4]=0$ except in the degenerate case where $X=0$ with probability 1 (which would be ruled out here as it would also imply that $\E[X^2]=0$).
    
## Extra Question

```{r}
load("fertilizer_2000.RData")


# part (a)
nrow(fertilizer_2000)

# part (b)
fertilizer_2000[21,]$country

# part (c)
mean_gdp <- mean(fertilizer_2000$avgdppc)
mean_gdp

# part (d)
above_avg_gdp <- subset(fertilizer_2000, avgdppc > mean_gdp)
mean(above_avg_gdp$prec)
```

